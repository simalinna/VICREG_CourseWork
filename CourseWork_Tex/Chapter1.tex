\section{Обзор существующих подходов}
\label{sec:Chapter1} \index{Chapter1}

% \todo[inline]{Обзор существующих подходов. Здесь нужно описать основные типы существующих методов: контрастное, неконтрастное обучение, кроме этого еще один распространенный подход - научить сеть-классификатор на большой базе и отрезать слои классификации, оставив только выделение признаков (для этого часто используются сети, обученные на ImageNet: VGG, ResNet  и т.п.). По каждому типу дайте пару примеров: ссылка на статью + одно предложение с описанием идеи. Укажите основные отличия этих подходов друг от друга.  Здесь можно ссылаться на методы, которые Вы дальше будете рассматривать подробно.
% Из обзора должна следовать постановка задачи: Вы обозначили, какие типы подходов есть, чем они друг от друга отличаются и из этого выбрали, что будете исследовать в своей работе.}

% описать основную идею метода
% привести пару примеров конкретных методов
% для каждого примера прикрепить сслыку на статью и написать одно предложение с описанием идеи
% написать основные отличия приведенных примеров

Среди методов извлечения признаков можно выделить три основных подхода.

\subsection{Методы, основанные на обучении без учителя}
\label{sec:methods}

Данная группа методов была предложена совсем недавно. Они не требует размеченных данных для обучения, при этом показывают высокое качество работы. Именно поэтому они имеют большое преимущество по сравнению с многими методами, которые использовались ранее для задачи извлечения признаков. 

Общая идея данного подхода заключается в выполнении следующих двух задач:
\begin{enumerate} 
    \item Сохранить инвариантность для всех положительных пар набора.
    \item Получать представления с декоррелированным пространством признаков.
\end{enumerate}

Понятие "положительная пара"\ имеет разное определение в зависимости от контекста задачи. Если речь идет про обучение с учителем, то положительной парой будем называть представления, полученные из разных объектов одного класса. Если же речь идет про обучение без учителя, как в нашем случае, то положительными парами будем называть два разных представления, полученных из одного и того же объекта путем аугментаций и прочих преобразований.  

Представления - это и есть наши целевые векторы, которые содержат извлеченные признаки.

Выполнение первой задачи необходимо, чтобы модель не извлекала шумовые признаки, и чтобы таким образом представления получались как можно более информативными.

Выполнение второй задачи тоже помогает повысить информативность представлений, поскольку декорреляция уменьшает избыточность признакового пространства. Однако это не единственная причина, по которой требуется выполнение данной задачи. Дело в том, что если выполнять только первую задачу инвариантности, то можно столкнуться с проблемой коллапса. Эта проблема заключается в том, что модель начинает игнорировать входные данные и создает идентичные и постоянные выходы для всех объектов. Другими словами, она находит тривиальное решение для выполнения инвариантности положительных пар, делая инвариантными все пары объектов. Однако если выполняется декорреляция признаков, то модель уже не может генерировать константные выходы. Таким образом с помощью декорреляции мы решаем сразу две подзадачи: уменьшаем избыточность признакового пространства и предотвращаем проблему коллапса.

Все методы данной группы очень похожи, но немного отличаются в реализации основной идеи. Например, в 2021 году был предложен метод Whitening-MSE \cite{Whitening_MSE}. Его особенностью является то, что для деккореляции признакового пространства он использует дополнительный модуль, который преобразует представления
в собственное пространство их ковариационной матрицы, и векторы, полученные таким образом, равномерно распределяются на единичную сферу. Однако такой подход влечет за собой громоздкие матричные вычисления, что является очень дорогостоящим использованием вычислительных ресурсов. Чуть позже в 2021 был предложен метод Barlow Twins \cite{Barlow_Twins}, который предлагает более простую реализацию. С помощью функции потерь он нормализует матрицу кросс-корреляции, в которой каждый элемент представляет собой коэффициент корреляции между выходами двух ветвей. Нормализация такой матрицы позволяет решить сразу две вышеописанные задачи. В январе 2022 года был предложен метод VICReg \cite{Vicreg}. Он заимствует механизм декорреляции у метода Barlow Twins. Однако вместо матрицы кросс-корреляции он использует матрицу ковариации, которая составляется для каждого из выходов ветвей. Данную матрицу он также нормализует с помощью функции потерь, и кроме этого использует дополнительные компоненты дисперсии и инвариантности.

\subsection{Контрастное обучение}
% \label{sec:begin}

Контрастное обучение по своей структуре схоже с методами, основанными на обучении без учителя, однако вместо декорреляции использует другой механизм. Общую идею также можно разбить на две задачи:
\begin{enumerate} 
    \item Сохранить инвариантность для всех положительных пар набора, минимизируя расстояние между ними.
    \item Максимизировать расстояние для всех отрицательных пар набора.
\end{enumerate}

Отрицательной парой, по аналогии с положительной, будем называть представления, полученные из объектов разных классов, если речь идет про обучение с учителем. Или же представления, полученные из разных объектов, если речь идет про обучение без учителя. 

Таким образом, в полученном пространстве представлений положительные пары будут располагаться близко, а отрицательные далеко. Данная идея позволяет получать информативные представления, избегая попадания шума, а также не позволяет сети "сломаться" и создавать идентичные представления для всех объектов, тем самым избегая проблемы коллапса.

Методы контрастного обучения дают хорошие результаты на практике. Однако для обучения им требуется большое количество контрастных пар. Как правило, данный подход подразумевает обучение с учителем. То есть эти пары задаются непосредственно в выборке, как например в методе SimCLR \cite{SimCLR}, предложенном в 2020 году. Но такой подход очень ресурсозатратный, поэтому существуют некоторые более сложные методы, которые способны обучаться без размеченных данных. Например метод MoCo \cite{MoCo}, предложенный ранее в 2020 году, как и методы из предыдущего раздела, использует аугментации для преобразований исходных объектов, а также банк памяти для хранения представлений всех объектов. За счет чего сам генерирует положительные и отрицательные пары. 

\subsection{Обучение с учителем}
% \label{sec:begin}

Стоит также рассказать про более примитивный подход, с которым в дальнейшем будут сравниваться рассмотренные выше методы. Несмотря на простую идею, он все же является довольно распространенным и используется на практике. 

Алгоритм работы данного методы можно описать следующим образом:
\begin{enumerate} 
    \item Обучить на большой базе объектов нейронную сеть для задачи классификации.
    \item Использовать обученную сеть без последнего слоя классификации для выделения признаков.
\end{enumerate}

Дело в том, что в процессе обучения нейронная сеть итак учится извлекать сложные и дискриминативные признаки, которые будут полезными для решения задачи. Поэтому представления, полученные из предпоследнего слоя, будут довольно информативными.  

В статьях \cite{lin2020training} и \cite{Mazumder_2021} описаны примеры использования данного метода для задачи выделения ключевых слов.

Конечно, предыдущие подходы, которые используют дополнительные вычисления для повышения информативности представлений, зачастую дают лучшие результаты на практике. Однако данный метод использует гораздо меньше вычислений при обучении.
