\section{Программная реализация}
\label{sec:Chapter5} \index{Chapter5}

Для программной реализации метода VICReg был использован язык Python 3.10.12. В качестве фреймворка была выбрана библиотека PyTorch 2.2.1. 

Для удобного написания кода была использована среда PyCharm. Также использовалась среда Google Collab для запуска программы с графическим ускорителем NVIDIA Tesla T4 и для красивой визуализации данных. 

\subsection{Структура программы}

Программа состоит из пяти модулей:
\begin{enumerate}
    \item Модуль для обучения метода VICReg;
    \item Модуль для тестирования метода VICReg;
    \item Модуль с описанием архитектуры энкодера;
    \item Модуль с аугментациями.
\end{enumerate}

Опишем подробнее каждый из них. 

В первом модуле начальным этапом происходит загрузка тренировочной и тестовой частей датасета. Датасет загружается из папки с файлами с помощью функции DataLoader. 

Далее создается объект класса VICREG. Данный класс описан в этом же модуле. Он наследуется от torch.nn.Module и переопределяет функцию forward. В этой функции происходит последовательный проход двух ветвей через энкодер и проектор, а также вычисляется функция ошибки:

\begin{lstlisting}[language=python, caption=Функция forward класса VICREG]
    def forward(self, x, y):
        
        x = self.projector(self.backbone(x))
        y = self.projector(self.backbone(y))

        var = variance(x) + variance(y)
        inv = invariance(x, y)
        cov = covariance(x) + covariance(y)

        var_coeff, inv_coeff, cov_coeff = 25, 25, 1

        loss = var_coeff*var + inv_coeff*inv + cov_coeff*cov

        return loss
\end{lstlisting}

\begin{lstlisting}[language=python, caption=Вычисления значений компонент функции потерь]
    def variance(z):
        return relu(1 - z.std(0)).mean()

    def invariance(x, y):
        return mse_loss(x, y)

    def covariance(z):
        n, d = z.shape
        m = z.mean(0)
        cov = torch.einsum("ni,nj->ij", z-m, z-m) / (n - 1)
        off_diag = cov.pow(2).sum() - cov.pow(2).diag().sum()
        return off_diag / d
\end{lstlisting}

Затем создаются необходимые файлы формата .json и .pth для сохранения значений функции ошибки на каждой эпохе и для сохранения обученных весов модели соответственно.

Последним этапом происходит обучение модели. В качестве оптимизатора используется Adam.

Во втором модуле первым этапом создается энкодер. Он должен иметь такую же архитектуру, которая использовалась в предыдущем модуле при обучении. С помощью метода load\_state\_dict в энкодер загружаются обученные веса. 

Далее создается модель, которая последовательно соединяет энкодер и однослойный линейный классификатор. Если тестирование проводится для линейной оценки, то с помощью функции requires\_grad\_ модель настраивается таким образом, чтобы веса энкодера были заморожены и обучался только линейный классификатор. Если же тестирование проводится для частичного обучения, то функция requires\_grad\_ не используется и обучается вся модель. Способ тестирования можно определить с помощью передачи аргумента командной строки. По умолчанию проводится линейная оценка.

Затем, аналогично первому модулю, создаются необходимые файлы, и происходит загрузка датасета.

Последним этапом происходит обучение классификатора. В качестве оптимизатора используется SGD, в качестве функции ошибки используется CrossEntropyLoss.

Третий модуль содержит классы, которые описывают вышеупомянутые архитектуры энкодера. Они, как и класс VICREG, наследуются от torch.nn.Module.

Четвертый модуль содержит аугментации, которые необходимы, чтобы разделить изначальную выборку на две ветви. Они написаны с помощью модуля torchvision.transforms.